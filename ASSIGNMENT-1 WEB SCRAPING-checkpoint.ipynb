{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68415106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mahar_9521u5e\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57159aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc141013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Header Tags\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "#Q1 Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    header_text = [tag.get_text() for tag in header_tags]\n",
    "    df = pd.DataFrame({'Header Tags': header_text})\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffc78d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "#Q2 Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "#from https://presidentofindia.nic.in/former-presidents.htm and make data frame\n",
    "\n",
    "\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'table table-bordered'})\n",
    "    names = []\n",
    "    terms = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].get_text(strip=True)\n",
    "        term = columns[1].get_text(strip=True)\n",
    "        names.append(\"name\")\n",
    "        terms.append(\"term\")\n",
    "\n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({'Name': names, 'Term of Office': terms})\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e02df600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men's cricket along with the records for matches, points and rating:\n",
      "        Team Matches Points Rating\n",
      "   Australia      27  3,112    115\n",
      "    Pakistan      27     27    115\n",
      "       India   3,102  3,102    114\n",
      "     England      40     40    105\n",
      "South Africa   4,558  4,558    104\n",
      " New Zealand      28     28    100\n",
      "  Bangladesh   2,942  2,942     94\n",
      "   Sri Lanka      23     23     93\n",
      " Afghanistan   2,386  2,386     80\n",
      " West Indies      31     31     68\n",
      "\n",
      "Top 10 ODI batsmen along with the records of their team and rating:\n",
      "              Batsman Team Rating\n",
      "           Babar Azam  PAK    863\n",
      "         Shubman Gill  IND    759\n",
      "Rassie van der Dussen   SA    745\n",
      "         David Warner  AUS    739\n",
      "          Imam-ul-Haq  PAK    735\n",
      "         Harry Tector  IRE    726\n",
      "      Quinton de Kock   SA    721\n",
      "          Virat Kohli  IND    715\n",
      "         Rohit Sharma  IND    707\n",
      "         Fakhar Zaman  PAK    705\n",
      "\n",
      "Top 10 ODI bowlers along with the records of their team and rating:\n",
      "          Bowler Team Rating\n",
      "  Josh Hazlewood  AUS    692\n",
      "  Mitchell Starc  AUS    666\n",
      "     Trent Boult   NZ    666\n",
      "      Adam Zampa  AUS    663\n",
      "      Matt Henry   NZ    658\n",
      "Mujeeb Ur Rahman  AFG    657\n",
      "   Kuldeep Yadav  IND    656\n",
      "     Rashid Khan  AFG    655\n",
      "  Mohammed Siraj  IND    643\n",
      "  Shaheen Afridi  PAK    635\n"
     ]
    }
   ],
   "source": [
    "#Q3 Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame                                                                                                     \n",
    "#a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "#c) Top 10 ODI bowlers along with the records of their team andrating\n",
    "\n",
    "# Scrape top 10 ODI teams in men's cricket along with the records for matches, points and rating\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "for team in soup.find_all('span', class_='u-hide-phablet'):\n",
    "    teams.append(team.text.strip())\n",
    "\n",
    "for match in soup.find_all('td', class_='rankings-block__banner--matches'):\n",
    "    matches.append(match.text.strip())\n",
    "\n",
    "for point in soup.find_all('td', class_='rankings-block__banner--points'):\n",
    "    points.append(point.text.strip())\n",
    "\n",
    "for rating in soup.find_all('td', class_='rankings-block__banner--rating u-text-right'):\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "for match, point, rating in zip(soup.find_all('td', class_='table-body__cell u-center-text'), soup.find_all('td', class_='table-body__cell u-center-text'), soup.find_all('td', class_='table-body__cell u-text-right rating')):\n",
    "    matches.append(match.text.strip())\n",
    "    points.append(point.text.strip())\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "df_teams = pd.DataFrame({'Team': teams[:10], 'Matches': matches[:10], 'Points': points[:10], 'Rating': ratings[:10]})\n",
    "\n",
    "# Scrape top 10 ODI batsmen along with the records of their team and rating\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "batsmen = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "for batsman in soup.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "    batsmen.append(batsman.text.strip())\n",
    "\n",
    "for team in soup.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "    teams.append(team.text.strip())\n",
    "\n",
    "for rating in soup.find_all('div', class_='rankings-block__banner--rating'):\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "for batsman, team, rating in zip(soup.find_all('td', class_='table-body__cell rankings-table__name name'), soup.find_all('span', class_='table-body__logo-text'), soup.find_all('td', class_='table-body__cell rating')):\n",
    "    batsmen.append(batsman.text.strip())\n",
    "    teams.append(team.text.strip())\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "df_batsmen = pd.DataFrame({'Batsman': batsmen[:10], 'Team': teams[:10], 'Rating': ratings[:10]})\n",
    "\n",
    "# Scrape top 10 ODI bowlers along with the records of their team and rating\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "bowlers = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "for bowler in soup.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "    bowlers.append(bowler.text.strip())\n",
    "\n",
    "for team in soup.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "    teams.append(team.text.strip())\n",
    "\n",
    "for rating in soup.find_all('div', class_='rankings-block__banner--rating'):\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "for bowler, team, rating in zip(soup.find_all('td', class_='table-body__cell rankings-table__name name'), soup.find_all('span', class_='table-body__logo-text'), soup.find_all('td', class_='table-body__cell rating')):\n",
    "    bowlers.append(bowler.text.strip())\n",
    "    teams.append(team.text.strip())\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "df_bowlers = pd.DataFrame({'Bowler': bowlers[:10], 'Team': teams[:10], 'Rating': ratings[:10]})\n",
    "\n",
    "print(\"Top 10 ODI teams in men's cricket along with the records for matches, points and rating:\")\n",
    "print(df_teams.to_string(index=False))\n",
    "print(\"\\nTop 10 ODI batsmen along with the records of their team and rating:\")\n",
    "print(df_batsmen.to_string(index=False))\n",
    "print(\"\\nTop 10 ODI bowlers along with the records of their team and rating:\")\n",
    "print(df_bowlers.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6f4d041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women's cricket along with the records for matches, points and rating:\n",
      "        Team Matches Points Rating\n",
      "   Australia      26  4,290    165\n",
      "     England      31     31    125\n",
      "South Africa   3,875  3,875    119\n",
      "       India      26     26    101\n",
      " New Zealand   3,098  3,098     96\n",
      " West Indies      30     30     95\n",
      "  Bangladesh   3,039  3,039     76\n",
      "   Sri Lanka      28     28     68\n",
      "    Thailand   2,688  2,688     68\n",
      "    Pakistan      29     29     62\n",
      "\n",
      "Top 10 women's ODI batting players along with the records of their team and rating:\n",
      "             Batsman Team Rating\n",
      "Natalie Sciver-Brunt  ENG    801\n",
      "         Beth Mooney  AUS    751\n",
      " Chamari Athapaththu   SL    743\n",
      "     Laura Wolvaardt   SA    708\n",
      "     Smriti Mandhana  IND    708\n",
      "        Alyssa Healy  AUS    702\n",
      "    Harmanpreet Kaur  IND    694\n",
      "        Ellyse Perry  AUS    686\n",
      "         Meg Lanning  AUS    682\n",
      "     Stafanie Taylor   WI    618\n",
      "\n",
      "Top 10 women's ODI all-rounders along with the records of their team and rating:\n",
      "         All-Rounder Team Rating\n",
      "Natalie Sciver-Brunt  ENG    398\n",
      "    Ashleigh Gardner  AUS    389\n",
      "     Hayley Matthews   WI    382\n",
      "      Marizanne Kapp   SA    362\n",
      "        Ellyse Perry  AUS    329\n",
      "         Amelia Kerr   NZ    328\n",
      "       Deepti Sharma  IND    312\n",
      "       Jess Jonassen  AUS    241\n",
      "       Sophie Devine   NZ    233\n",
      "            Nida Dar  PAK    217\n"
     ]
    }
   ],
   "source": [
    "#Q4 Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#c) Top 10 women’s ODI all-rounder along with the records of their team and rating\n",
    "\n",
    "# Scrape top 10 ODI teams in women's cricket along with the records for matches, points and rating\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "teams = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "for team in soup.find_all('span', class_='u-hide-phablet'):\n",
    "    teams.append(team.text.strip())\n",
    "\n",
    "for match in soup.find_all('td', class_='rankings-block__banner--matches'):\n",
    "    matches.append(match.text.strip())\n",
    "\n",
    "for point in soup.find_all('td', class_='rankings-block__banner--points'):\n",
    "    points.append(point.text.strip())\n",
    "\n",
    "for rating in soup.find_all('td', class_='rankings-block__banner--rating u-text-right'):\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "for match, point, rating in zip(soup.find_all('td', class_='table-body__cell u-center-text'), soup.find_all('td', class_='table-body__cell u-center-text'), soup.find_all('td', class_='table-body__cell u-text-right rating')):\n",
    "    matches.append(match.text.strip())\n",
    "    points.append(point.text.strip())\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "df_teams = pd.DataFrame({'Team': teams[:10], 'Matches': matches[:10], 'Points': points[:10], 'Rating': ratings[:10]})\n",
    "\n",
    "# Scrape top 10 ODI batsmen along with the records of their team and rating\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "batsmen = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "for batsman in soup.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "    batsmen.append(batsman.text.strip())\n",
    "\n",
    "for team in soup.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "    teams.append(team.text.strip())\n",
    "\n",
    "for rating in soup.find_all('div', class_='rankings-block__banner--rating'):\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "for batsman, team, rating in zip(soup.find_all('td', class_='table-body__cell rankings-table__name name'), soup.find_all('span', class_='table-body__logo-text'), soup.find_all('td', class_='table-body__cell rating')):\n",
    "    batsmen.append(batsman.text.strip())\n",
    "    teams.append(team.text.strip())\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "df_batsmen = pd.DataFrame({'Batsman': batsmen[:10], 'Team': teams[:10], 'Rating': ratings[:10]})\n",
    "\n",
    "# Scrape top 10 ODI all-rounders along with the records of their team and rating\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "all_rounders = []\n",
    "teams = []\n",
    "ratings = []\n",
    "\n",
    "for all_rounder in soup.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "    all_rounders.append(all_rounder.text.strip())\n",
    "\n",
    "for team in soup.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "    teams.append(team.text.strip())\n",
    "\n",
    "for rating in soup.find_all('div', class_='rankings-block__banner--rating'):\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "for all_rounder, team, rating in zip(soup.find_all('td', class_='table-body__cell rankings-table__name name'), soup.find_all('span', class_='table-body__logo-text'), soup.find_all('td', class_='table-body__cell rating')):\n",
    "    all_rounders.append(all_rounder.text.strip())\n",
    "    teams.append(team.text.strip())\n",
    "    ratings.append(rating.text.strip())\n",
    "\n",
    "df_all_rounders = pd.DataFrame({'All-Rounder': all_rounders[:10], 'Team': teams[:10], 'Rating': ratings[:10]})\n",
    "\n",
    "print(\"Top 10 ODI teams in women's cricket along with the records for matches, points and rating:\")\n",
    "print(df_teams.to_string(index=False))\n",
    "print(\"\\nTop 10 women's ODI batting players along with the records of their team and rating:\")\n",
    "print(df_batsmen.to_string(index=False))\n",
    "print(\"\\nTop 10 women's ODI all-rounders along with the records of their team and rating:\")\n",
    "print(df_all_rounders.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1f3550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline           Time  \\\n",
      "0   U.S. ambassador to Russia visits jailed report...  Not available   \n",
      "1   Poland, Hungary, Slovakia to introduce own ban...  Not available   \n",
      "2   Senators ask Pentagon for answers on SpaceX’s ...  Not available   \n",
      "3   UK designates Wagner Group as terrorists; Russ...  Not available   \n",
      "4   With outlaws for allies, is Russia becoming an...  Not available   \n",
      "5   Shipping giant Maersk unveils world's first ve...  Not available   \n",
      "6   Major UN report offers damning assessment of t...  Not available   \n",
      "7   Countries are refinancing debt into ocean cons...  Not available   \n",
      "8   We have to work with the private sector: The N...  Not available   \n",
      "9   $700 billion a year is needed to reverse the d...  Not available   \n",
      "10  Thai hospitality magnate is 'very optimistic' ...  Not available   \n",
      "11  The Middle East has become the 'hub for the wo...  Not available   \n",
      "12  ASEAN ‘at a loss for ideas’ on how to handle M...  Not available   \n",
      "13  Our estates have distinguished themselves: Mya...  Not available   \n",
      "14  Philippines' Globe Telecom CEO on how AI can b...  Not available   \n",
      "15  An Italian island is letting foreigners live r...  Not available   \n",
      "16  Years in the making, Venice approves a tax on ...  Not available   \n",
      "17  Watch a hotel room transform into a $62,000 su...  Not available   \n",
      "18  Where to stay in India? Here are 8 former pala...  Not available   \n",
      "19  Wall, what wall? Two detained after leveling p...  Not available   \n",
      "20  4 things the world's longest-living people do ...  Not available   \n",
      "21  How Olipop's founders started a soda brand bri...  Not available   \n",
      "22  U.S. states where property taxes are highest—N...  Not available   \n",
      "23  We built Olipop: A $20 million a month soda co...  Not available   \n",
      "24  The No. 1 thing successful parents who raise t...  Not available   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/09/16/us-ambassador-...  \n",
      "1   https://www.cnbc.com/2023/09/16/poland-hungary...  \n",
      "2   https://www.cnbc.com/2023/09/15/spacex-starlin...  \n",
      "3   https://www.cnbc.com/2023/09/15/live-updates-l...  \n",
      "4   https://www.cnbc.com/2023/09/15/is-russia-a-ro...  \n",
      "5   https://www.cnbc.com/2023/09/14/shipping-giant...  \n",
      "6   https://www.cnbc.com/2023/09/08/climate-un-rep...  \n",
      "7   https://www.cnbc.com/video/2023/09/14/countrie...  \n",
      "8   https://www.cnbc.com/video/2023/09/14/we-have-...  \n",
      "9   https://www.cnbc.com/video/2023/09/14/700-bill...  \n",
      "10  https://www.cnbc.com/2023/09/13/thai-hospitali...  \n",
      "11  https://www.cnbc.com/video/2023/09/11/middle-e...  \n",
      "12  https://www.cnbc.com/2023/09/05/asean-at-a-los...  \n",
      "13  https://www.cnbc.com/video/2023/09/11/yoma-str...  \n",
      "14  https://www.cnbc.com/video/2023/09/04/philippi...  \n",
      "15  https://www.cnbc.com/2023/09/15/digital-nomads...  \n",
      "16  https://www.cnbc.com/2023/09/13/new-tax-to-vis...  \n",
      "17  https://www.cnbc.com/video/2023/09/11/watch-a-...  \n",
      "18  https://www.cnbc.com/2023/09/08/where-to-stay-...  \n",
      "19  https://www.cnbc.com/2023/09/07/great-wall-of-...  \n",
      "20  https://www.cnbc.com/2023/09/16/4-things-the-w...  \n",
      "21  https://www.cnbc.com/2023/09/16/how-olipops-fo...  \n",
      "22  https://www.cnbc.com/2023/09/16/us-states-wher...  \n",
      "23  https://www.cnbc.com/video/2023/09/16/we-built...  \n",
      "24  https://www.cnbc.com/2023/09/16/the-no-1-paren...  \n"
     ]
    }
   ],
   "source": [
    "#Q5 Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "#make data frame\n",
    "#i) Headline\n",
    "#ii) Time\n",
    "#iii) News Link\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "for article in soup.find_all('div', class_='Card-titleContainer'):\n",
    "    headline = article.find('a').text.strip()\n",
    "    time_tag = article.find('time')\n",
    "\n",
    "    if time_tag and 'datetime' in time_tag.attrs:\n",
    "        time = time_tag['datetime']\n",
    "    else:\n",
    "        time = \"Not available\"\n",
    "\n",
    "    link = article.find('a')['href']\n",
    "\n",
    "    headlines.append(headline)\n",
    "    times.append(time)\n",
    "    links.append(link)\n",
    "\n",
    "df = pd.DataFrame({'Headline': headlines, 'Time': times, 'News Link': links})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4bff755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q6 Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "#days.https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "#Scrape below mentioned details and make data frame\n",
    "#i) Paper Title\n",
    "#ii) Authors\n",
    "#iii) Published Date\n",
    "#iv) Paper URL\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "for article in soup.find_all('div', class_='block-link__overlay-link'):\n",
    "    titles.append(article.find('span', class_='js-headline-text').text)\n",
    "    authors.append(article.find('span', class_='byline__author-name').text)\n",
    "    dates.append(article.find('time')['datetime'])\n",
    "    urls.append(article['href'])\n",
    "\n",
    "df = pd.DataFrame({'Paper Title': titles, 'Authors': authors, 'Published Date': dates, 'Paper URL': urls})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9457329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name  \\\n",
      "0                   Castle Barbeque   \n",
      "1                        Cafe Knosh   \n",
      "2                 Castle's Barbeque   \n",
      "3                       India Grill   \n",
      "4              The Barbeque Company   \n",
      "5                    Delhi Barbeque   \n",
      "6  The Monarch - Bar Be Que Village   \n",
      "7                 Indian Grill Room   \n",
      "8                The Barbeque Times   \n",
      "\n",
      "                                             Cuisine       Location  \\\n",
      "0                     Connaught Place, Central Delhi  Not available   \n",
      "1  The Leela Ambience Convention Hotel,Shahdara, ...  Not available   \n",
      "2             Pacific Mall,Tagore Garden, West Delhi  Not available   \n",
      "3               Hilton Garden Inn,Saket, South Delhi  Not available   \n",
      "4                 Gardens Galleria,Sector 38A, Noida  Not available   \n",
      "5     Taurus Sarovar Portico,Mahipalpur, South Delhi  Not available   \n",
      "6  Indirapuram Habitat Centre,Indirapuram, Ghaziabad  Not available   \n",
      "7   Suncity Business Tower,Golf Course Road, Gurgaon  Not available   \n",
      "8              M2K Corporate Park,Sector 51, Gurgaon  Not available   \n",
      "\n",
      "         Ratings      Image URL  \n",
      "0  Not available  Not available  \n",
      "1  Not available  Not available  \n",
      "2  Not available  Not available  \n",
      "3  Not available  Not available  \n",
      "4  Not available  Not available  \n",
      "5  Not available  Not available  \n",
      "6  Not available  Not available  \n",
      "7  Not available  Not available  \n",
      "8  Not available  Not available  \n"
     ]
    }
   ],
   "source": [
    "#Q7  Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "#i) Restaurant name\n",
    "#ii) Cuisine\n",
    "#iii) Location\n",
    "#iv) Ratings\n",
    "#v) Image URL\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "results = soup.find_all('div', class_='restnt-card restaurant')\n",
    "\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "for result in results:\n",
    "    name = result.find('a', class_='restnt-name ellipsis').text.strip()\n",
    "    \n",
    "    # Use the 'div' element with class 'restnt-loc' for both cuisine and location\n",
    "    cuisine_and_location = result.find('div', class_='restnt-loc').text.strip()\n",
    "    \n",
    "    # Split the cuisine_and_location text to separate cuisine and location if '|' is present\n",
    "    if '|' in cuisine_and_location:\n",
    "        cuisine, location = map(str.strip, cuisine_and_location.split('|', 1))\n",
    "    else:\n",
    "        cuisine = cuisine_and_location\n",
    "        location = \"Not available\"\n",
    "    \n",
    "    # Check if rating element exists before accessing its text\n",
    "    rating_element = result.find('span', class_='rating-val')\n",
    "    rating = rating_element.text.strip() if rating_element else \"Not available\"\n",
    "    \n",
    "    try:\n",
    "        image_url = result.find('img')['src']\n",
    "    except KeyError:\n",
    "        # Handle the case where the 'src' attribute is not present\n",
    "        image_url = \"Not available\"\n",
    "\n",
    "    restaurant_names.append(name)\n",
    "    cuisines.append(cuisine)\n",
    "    locations.append(location)\n",
    "    ratings.append(rating)\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Restaurant Name': restaurant_names,\n",
    "    'Cuisine': cuisines,\n",
    "    'Location': locations,\n",
    "    'Ratings': ratings,\n",
    "    'Image URL': image_urls\n",
    "})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba6f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae135e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71855586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
